{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (from scipy) (1.19.4)\n",
      "Requirement already satisfied: scikit-learn in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (0.23.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (from scikit-learn) (1.19.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (from scikit-learn) (0.17.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (from scikit-learn) (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (from scikit-learn) (1.19.4)\n",
      "Requirement already satisfied: statsmodels in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (0.12.1)\n",
      "Requirement already satisfied: pandas>=0.21 in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (from statsmodels) (1.1.5)\n",
      "Requirement already satisfied: patsy>=0.5 in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (from statsmodels) (0.5.1)\n",
      "Requirement already satisfied: scipy>=1.1 in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (from statsmodels) (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (from statsmodels) (1.19.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (from pandas>=0.21->statsmodels) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (from statsmodels) (1.19.4)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (from pandas>=0.21->statsmodels) (2020.4)\n",
      "Requirement already satisfied: six in /home/aidev/.local/lib/python3.6/site-packages (from patsy>=0.5->statsmodels) (1.13.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (from statsmodels) (1.19.4)\n",
      "Requirement already satisfied: six in /home/aidev/.local/lib/python3.6/site-packages (from patsy>=0.5->statsmodels) (1.13.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (from statsmodels) (1.19.4)\n",
      "Requirement already satisfied: pandas in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (1.1.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (from pandas) (2020.4)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (from pandas) (1.19.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/aidev/.local/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.13.0)\n",
      "Requirement already satisfied: numpy in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (1.19.4)\n",
      "Requirement already satisfied: lightgbm in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (3.2.1)\n",
      "Requirement already satisfied: wheel in /home/aidev/.local/lib/python3.6/site-packages (from lightgbm) (0.33.6)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (from lightgbm) (0.23.2)\n",
      "Requirement already satisfied: scipy in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (from lightgbm) (1.5.4)\n",
      "Requirement already satisfied: numpy in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (from lightgbm) (1.19.4)\n",
      "Requirement already satisfied: scipy in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (from lightgbm) (1.5.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (from scikit-learn!=0.22.0->lightgbm) (2.1.0)\n",
      "Requirement already satisfied: numpy in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (from lightgbm) (1.19.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (from scikit-learn!=0.22.0->lightgbm) (0.17.0)\n",
      "Requirement already satisfied: numpy in /home/aidev/anaconda3/envs/tf2/lib/python3.6/site-packages (from lightgbm) (1.19.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy\n",
    "!pip install scikit-learn\n",
    "!pip install statsmodels\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import pandas\n",
    "import ast\n",
    "import csv\n",
    "import lightgbm as lgb\n",
    "from sklearn.datasets import load_breast_cancer,load_boston,load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import mean_squared_error,roc_auc_score,precision_score\n",
    "pd.options.display.max_columns = 999\n",
    "d_type={}\n",
    "d_type['CALL']=0\n",
    "d_type['CREATE']=1\n",
    "d_type['STATICALL']=2\n",
    "d_error={'execution reverted': 0,\n",
    " 'Out of gas': 1,\n",
    " 'Bad instruction': 2,\n",
    " 'Bad jump destination': 3,\n",
    "  -1:-1}\n",
    "def find(d,key,none_case,inout=False,get_len=False):\n",
    "    if inout:\n",
    "        try:\n",
    "            return int(d[key][:10],0)%(2**63)\n",
    "        except:\n",
    "            return none_case\n",
    "    if get_len:\n",
    "        try:\n",
    "            return d[key]\n",
    "        except:\n",
    "            return none_case\n",
    "    try:\n",
    "        return int(d[key],0)#%(2**63)\n",
    "    except:\n",
    "        return none_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal(txTrace,txData,examples,label0,label1,blob,blob_trace):\n",
    "    gasUsed=find(txTrace,\"gasUsed\",-1)\n",
    "    output=find(txTrace,\"output\",-1,True)\n",
    "    inputs=find(txTrace,\"input\",-1,True)\n",
    "    try:\n",
    "        call_lens=len(find(txTrace,\"calls\",0,get_len=True))\n",
    "    except:\n",
    "        call_lens=0\n",
    "    froms=find(txTrace,\"from\",0)%(2**63)\n",
    "    gas=find(txTrace,\"gas\",-1)\n",
    "    to=find(txTrace,\"to\",0)%(2**63)\n",
    "    gasPrice=find(txData,\"gasPrice\",-1)\n",
    "#     len_trace=len(txTrace)\n",
    "    nonce=find(txData,\"nonce\",-1)\n",
    "#     types=d_type[find(txTrace,\"type\",0,get_len=True)]\n",
    "#     blocknumber=find(txData,\"blockNumber\",-1)\n",
    "    blocknumber=int(txTrace['blockNumber'])\n",
    "    value=float(find(txTrace,\"value\",-1))\n",
    "#     error=d_error[find(txTrace,\"error\",-1,get_len=True)]\n",
    "    delegate=blob_trace.count(\"DELEGATECALL\")\n",
    "    statics=blob_trace.count(\"STATICCALL\")\n",
    "    \n",
    "#     if value!=-1:\n",
    "#         value=value%(2**30)\n",
    "    if label1!=None:\n",
    "        examples.append([froms,to,gas,gasPrice,inputs,nonce,blocknumber,value,gasUsed,call_lens,output,delegate,statics,int(label0),float(label1)])\n",
    "#         examples.append([froms,to,gas,inputs,blocknumber,value,gasUsed,len_trace,call_lens,output,label0,label1])\n",
    "    else:\n",
    "        examples.append([froms,to,gas,gasPrice,inputs,nonce,blocknumber,value,gasUsed,call_lens,output,delegate,statics])\n",
    "#         examples.append([froms,to,gas,inputs,blocknumber,value,gasUsed,len_trace,call_lens,output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter training csvtrain.csv\n",
      "Please enter testing csvtest.csv\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "118000\n",
      "119000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "123000\n",
      "124000\n",
      "125000\n",
      "126000\n",
      "127000\n",
      "128000\n",
      "129000\n",
      "130000\n",
      "131000\n",
      "132000\n",
      "133000\n",
      "134000\n",
      "135000\n",
      "136000\n",
      "137000\n",
      "138000\n",
      "139000\n",
      "140000\n",
      "141000\n",
      "142000\n",
      "143000\n",
      "144000\n",
      "145000\n",
      "146000\n",
      "147000\n",
      "148000\n",
      "149000\n",
      "150000\n",
      "151000\n",
      "152000\n",
      "153000\n",
      "154000\n",
      "155000\n",
      "156000\n",
      "157000\n",
      "158000\n",
      "159000\n",
      "160000\n",
      "161000\n",
      "162000\n",
      "163000\n",
      "164000\n",
      "165000\n",
      "166000\n",
      "167000\n",
      "168000\n",
      "169000\n",
      "170000\n",
      "171000\n",
      "172000\n",
      "173000\n",
      "174000\n",
      "175000\n",
      "176000\n",
      "177000\n",
      "178000\n",
      "179000\n",
      "180000\n",
      "181000\n",
      "182000\n",
      "183000\n",
      "184000\n",
      "185000\n",
      "186000\n",
      "187000\n",
      "188000\n",
      "189000\n",
      "190000\n",
      "191000\n",
      "192000\n",
      "193000\n",
      "194000\n",
      "195000\n",
      "196000\n",
      "197000\n",
      "198000\n",
      "199000\n",
      "200000\n",
      "201000\n",
      "202000\n",
      "203000\n",
      "204000\n",
      "205000\n",
      "206000\n",
      "207000\n",
      "208000\n",
      "209000\n",
      "210000\n",
      "211000\n",
      "212000\n",
      "213000\n",
      "214000\n",
      "215000\n",
      "216000\n",
      "217000\n",
      "218000\n",
      "219000\n",
      "220000\n",
      "221000\n",
      "222000\n",
      "223000\n",
      "224000\n",
      "225000\n",
      "226000\n",
      "227000\n",
      "228000\n",
      "229000\n",
      "230000\n",
      "231000\n",
      "232000\n",
      "233000\n",
      "234000\n",
      "235000\n",
      "236000\n",
      "237000\n",
      "238000\n",
      "239000\n",
      "240000\n",
      "241000\n",
      "242000\n",
      "243000\n",
      "244000\n",
      "245000\n",
      "246000\n",
      "247000\n",
      "248000\n",
      "249000\n",
      "250000\n",
      "251000\n",
      "252000\n",
      "253000\n",
      "254000\n",
      "255000\n",
      "256000\n",
      "257000\n",
      "258000\n",
      "259000\n",
      "260000\n",
      "261000\n",
      "262000\n",
      "263000\n",
      "264000\n",
      "265000\n",
      "266000\n",
      "267000\n",
      "268000\n",
      "269000\n",
      "270000\n",
      "271000\n",
      "272000\n",
      "273000\n",
      "274000\n",
      "275000\n",
      "276000\n",
      "277000\n",
      "278000\n",
      "279000\n",
      "280000\n",
      "281000\n",
      "282000\n",
      "283000\n",
      "284000\n",
      "285000\n",
      "286000\n",
      "287000\n",
      "288000\n",
      "289000\n",
      "290000\n",
      "291000\n",
      "292000\n",
      "293000\n",
      "294000\n",
      "295000\n",
      "296000\n",
      "297000\n",
      "298000\n",
      "299000\n",
      "300000\n",
      "301000\n",
      "302000\n",
      "303000\n",
      "304000\n",
      "305000\n",
      "306000\n",
      "307000\n",
      "308000\n",
      "309000\n",
      "310000\n",
      "311000\n",
      "312000\n",
      "313000\n",
      "314000\n",
      "315000\n",
      "316000\n",
      "317000\n",
      "318000\n",
      "319000\n",
      "320000\n",
      "321000\n",
      "322000\n",
      "323000\n",
      "324000\n",
      "325000\n",
      "326000\n",
      "327000\n",
      "328000\n",
      "329000\n",
      "330000\n",
      "331000\n",
      "332000\n",
      "333000\n",
      "334000\n",
      "335000\n",
      "336000\n",
      "337000\n",
      "338000\n",
      "339000\n",
      "340000\n",
      "341000\n",
      "342000\n",
      "343000\n",
      "344000\n",
      "345000\n",
      "346000\n",
      "347000\n",
      "348000\n",
      "349000\n",
      "350000\n",
      "351000\n",
      "352000\n",
      "353000\n",
      "354000\n",
      "355000\n",
      "356000\n",
      "357000\n",
      "358000\n",
      "359000\n",
      "360000\n",
      "361000\n",
      "362000\n",
      "363000\n",
      "364000\n",
      "365000\n",
      "366000\n",
      "367000\n",
      "368000\n",
      "369000\n",
      "370000\n",
      "371000\n",
      "372000\n",
      "373000\n",
      "374000\n",
      "375000\n",
      "376000\n",
      "377000\n",
      "378000\n",
      "379000\n",
      "380000\n",
      "381000\n",
      "382000\n",
      "383000\n",
      "384000\n",
      "385000\n",
      "386000\n",
      "387000\n",
      "388000\n",
      "389000\n",
      "390000\n",
      "391000\n",
      "392000\n",
      "393000\n",
      "394000\n",
      "395000\n",
      "396000\n",
      "397000\n",
      "398000\n",
      "399000\n",
      "400000\n",
      "401000\n",
      "402000\n",
      "403000\n",
      "404000\n",
      "405000\n",
      "406000\n",
      "407000\n",
      "408000\n",
      "409000\n",
      "410000\n",
      "411000\n",
      "412000\n",
      "413000\n",
      "414000\n",
      "415000\n",
      "416000\n",
      "417000\n",
      "418000\n",
      "419000\n",
      "420000\n",
      "421000\n",
      "422000\n",
      "423000\n",
      "424000\n",
      "425000\n",
      "426000\n",
      "427000\n",
      "428000\n",
      "429000\n",
      "430000\n",
      "431000\n",
      "432000\n",
      "433000\n",
      "434000\n",
      "435000\n",
      "436000\n",
      "437000\n",
      "438000\n",
      "439000\n",
      "440000\n",
      "441000\n",
      "442000\n",
      "443000\n",
      "444000\n",
      "445000\n",
      "446000\n",
      "447000\n",
      "448000\n",
      "449000\n",
      "450000\n",
      "451000\n",
      "452000\n",
      "453000\n",
      "454000\n",
      "455000\n",
      "456000\n",
      "457000\n",
      "458000\n",
      "459000\n",
      "460000\n",
      "461000\n",
      "462000\n",
      "463000\n",
      "464000\n",
      "465000\n",
      "466000\n",
      "467000\n",
      "468000\n",
      "469000\n",
      "470000\n",
      "471000\n",
      "472000\n",
      "473000\n",
      "474000\n",
      "475000\n",
      "476000\n",
      "477000\n",
      "478000\n",
      "479000\n",
      "480000\n",
      "481000\n",
      "482000\n",
      "483000\n",
      "484000\n",
      "485000\n",
      "486000\n",
      "487000\n",
      "488000\n",
      "489000\n",
      "490000\n",
      "491000\n",
      "492000\n",
      "493000\n",
      "494000\n",
      "495000\n",
      "496000\n",
      "497000\n",
      "498000\n",
      "499000\n",
      "500000\n",
      "501000\n",
      "502000\n",
      "503000\n",
      "504000\n",
      "505000\n",
      "506000\n",
      "507000\n",
      "508000\n",
      "509000\n",
      "510000\n",
      "511000\n",
      "512000\n",
      "513000\n",
      "514000\n",
      "515000\n",
      "516000\n",
      "517000\n",
      "518000\n",
      "519000\n",
      "520000\n",
      "521000\n",
      "522000\n",
      "523000\n",
      "524000\n",
      "525000\n",
      "526000\n",
      "527000\n",
      "528000\n",
      "529000\n",
      "530000\n",
      "531000\n",
      "532000\n",
      "533000\n",
      "534000\n",
      "535000\n",
      "536000\n",
      "537000\n",
      "538000\n",
      "539000\n",
      "540000\n",
      "541000\n",
      "542000\n",
      "543000\n",
      "544000\n",
      "545000\n",
      "546000\n",
      "547000\n",
      "548000\n",
      "549000\n",
      "550000\n",
      "551000\n",
      "552000\n",
      "553000\n",
      "554000\n",
      "555000\n",
      "556000\n",
      "557000\n",
      "558000\n",
      "559000\n",
      "560000\n",
      "561000\n",
      "562000\n",
      "563000\n",
      "564000\n",
      "565000\n",
      "566000\n",
      "567000\n",
      "568000\n",
      "569000\n",
      "570000\n",
      "571000\n",
      "572000\n",
      "573000\n",
      "574000\n",
      "575000\n",
      "576000\n",
      "577000\n",
      "578000\n",
      "579000\n",
      "580000\n",
      "581000\n",
      "582000\n",
      "583000\n",
      "584000\n",
      "585000\n",
      "586000\n",
      "587000\n",
      "588000\n",
      "589000\n",
      "590000\n",
      "591000\n",
      "592000\n",
      "593000\n",
      "594000\n",
      "595000\n",
      "596000\n",
      "597000\n",
      "598000\n",
      "599000\n",
      "600000\n",
      "601000\n",
      "602000\n",
      "603000\n",
      "604000\n",
      "605000\n",
      "606000\n",
      "607000\n",
      "608000\n",
      "609000\n",
      "610000\n",
      "611000\n",
      "612000\n",
      "613000\n",
      "614000\n",
      "615000\n",
      "616000\n",
      "617000\n",
      "618000\n",
      "619000\n",
      "620000\n",
      "621000\n",
      "622000\n",
      "623000\n",
      "624000\n",
      "625000\n",
      "626000\n",
      "627000\n",
      "628000\n",
      "629000\n",
      "630000\n",
      "631000\n",
      "632000\n",
      "633000\n",
      "634000\n",
      "635000\n",
      "636000\n",
      "637000\n",
      "638000\n",
      "639000\n",
      "640000\n",
      "641000\n",
      "642000\n",
      "643000\n",
      "644000\n",
      "645000\n",
      "646000\n",
      "647000\n",
      "648000\n",
      "649000\n",
      "650000\n",
      "651000\n",
      "652000\n",
      "653000\n",
      "654000\n",
      "655000\n",
      "656000\n",
      "657000\n",
      "658000\n",
      "659000\n",
      "660000\n",
      "661000\n",
      "662000\n",
      "663000\n",
      "664000\n",
      "665000\n",
      "666000\n",
      "667000\n",
      "668000\n",
      "669000\n",
      "670000\n",
      "671000\n",
      "672000\n",
      "673000\n",
      "674000\n",
      "675000\n",
      "676000\n",
      "677000\n",
      "678000\n",
      "679000\n",
      "680000\n",
      "681000\n",
      "682000\n",
      "683000\n",
      "684000\n",
      "685000\n",
      "686000\n",
      "687000\n",
      "688000\n",
      "689000\n",
      "690000\n",
      "691000\n",
      "692000\n",
      "693000\n",
      "694000\n",
      "695000\n",
      "696000\n",
      "697000\n",
      "698000\n",
      "699000\n",
      "700000\n",
      "701000\n",
      "702000\n",
      "703000\n",
      "704000\n",
      "705000\n",
      "706000\n",
      "707000\n",
      "708000\n",
      "709000\n",
      "710000\n",
      "711000\n",
      "712000\n",
      "713000\n",
      "714000\n",
      "715000\n",
      "716000\n",
      "717000\n",
      "718000\n",
      "719000\n",
      "720000\n",
      "721000\n",
      "722000\n",
      "723000\n",
      "724000\n",
      "725000\n",
      "726000\n",
      "727000\n",
      "728000\n",
      "729000\n",
      "730000\n",
      "731000\n",
      "732000\n",
      "733000\n",
      "734000\n",
      "735000\n",
      "736000\n",
      "737000\n",
      "738000\n",
      "739000\n",
      "740000\n",
      "741000\n",
      "742000\n",
      "743000\n",
      "744000\n",
      "745000\n",
      "746000\n",
      "747000\n",
      "748000\n",
      "749000\n",
      "750000\n",
      "751000\n",
      "752000\n",
      "753000\n",
      "754000\n",
      "755000\n",
      "756000\n",
      "757000\n",
      "758000\n",
      "759000\n",
      "760000\n",
      "761000\n",
      "762000\n",
      "763000\n",
      "764000\n",
      "765000\n",
      "766000\n",
      "767000\n",
      "768000\n",
      "769000\n",
      "770000\n",
      "771000\n",
      "772000\n",
      "773000\n",
      "774000\n",
      "775000\n",
      "776000\n",
      "777000\n",
      "778000\n",
      "779000\n",
      "780000\n",
      "781000\n",
      "782000\n",
      "783000\n",
      "784000\n",
      "785000\n",
      "786000\n",
      "787000\n",
      "788000\n",
      "789000\n",
      "790000\n",
      "791000\n",
      "792000\n",
      "793000\n",
      "794000\n",
      "795000\n",
      "796000\n",
      "797000\n",
      "798000\n",
      "799000\n",
      "800000\n",
      "801000\n",
      "802000\n",
      "803000\n",
      "804000\n",
      "805000\n",
      "806000\n",
      "807000\n",
      "808000\n",
      "809000\n",
      "810000\n",
      "811000\n",
      "812000\n",
      "813000\n",
      "814000\n",
      "815000\n",
      "816000\n",
      "817000\n",
      "818000\n",
      "819000\n",
      "820000\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def convert_dataset(dataset,examples,train=True):\n",
    "    for i in range(len(dataset)):\n",
    "        blob=dataset['txData'][i]\n",
    "        blob_trace=dataset['txTrace'][i]\n",
    "        txData = ast.literal_eval(blob)\n",
    "        txTrace = ast.literal_eval(blob_trace)\n",
    "#         if train==True and i in X_test_indexes:\n",
    "#             print(i,\"---skipped--\")\n",
    "#             continue\n",
    "        if i%1000==0:\n",
    "            print(i)\n",
    "        label0,label1=None,None\n",
    "        if train:\n",
    "            label0,label1=dataset['Label0'][i],dataset['Label1'][i]\n",
    "        cal(txTrace,txData,examples,label0,label1,blob,blob_trace)\n",
    "#         if train==False:\n",
    "#             continue\n",
    "#         try:\n",
    "#             call_lens=len(find(txTrace,\"calls\",0,get_len=True))\n",
    "#         except:\n",
    "#             call_lens=0\n",
    "#         if call_lens==0:\n",
    "#             continue\n",
    "#         for call in range(1,len(find(txTrace,\"calls\",0,get_len=True))-1):\n",
    "#             call=find(txTrace,\"calls\",0,get_len=True)[call]\n",
    "#             cal(call,txData,examples,True,label1)\n",
    "        \n",
    "    return np.array(examples)\n",
    "examples_train=[]\n",
    "examples_test=[]\n",
    "train_file=input(\"Please enter training csv\")\n",
    "test_file=input(\"Please enter testing csv\")\n",
    "train = pandas.read_csv('train.csv')\n",
    "test = pandas.read_csv('test.csv')\n",
    "train_data=convert_dataset(train,examples_train)\n",
    "test_data=convert_dataset(test,examples_test,train=False)\n",
    "train_df=pd.DataFrame(train_data,columns=[\"from\",\"to\",\"gas\",\"gasPrice\",\"input\",\"nonce\",\"blockNumber\",\"value\",\"gasUsed\",\"call_lens\",\"output\",\"delegate\",\"statics\",\"Label0\",\"Label1\"])\n",
    "# train_df=pd.DataFrame(train_data,columns=[\"from\",\"to\",\"gas\",\"input\",\"blockNumber\",\"value\",\"gasUsed\",\"len_trace\",\"call_lens\",\"output\",\"Label0\",\"Label1\"])\n",
    "\n",
    "\n",
    "test_df=pd.DataFrame(test_data,columns=[\"from\",\"to\",\"gas\",\"gasPrice\",\"input\",\"nonce\",\"blockNumber\",\"value\",\"gasUsed\",\"call_lens\",\"output\",\"delegate\",\"statics\"])\n",
    "# test_df=pd.DataFrame(test_data,columns=[\"from\",\"to\",\"gas\",\"input\",\"blockNumber\",\"value\",\"gasUsed\",\"len_trace\",\"call_lens\",\"output\"])\n",
    "\n",
    "\n",
    "train_df=train_df.apply(pd.to_numeric)\n",
    "test_df=test_df.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label0\n",
      "Label1\n",
      "Added new magic features!\n"
     ]
    }
   ],
   "source": [
    "def encode_FE(df,col,test):\n",
    "    cv = df[col].value_counts()\n",
    "    nm = col+'_FE'\n",
    "    df[nm] = df[col].map(cv)\n",
    "    test[nm] = test[col].map(cv)\n",
    "#     test[nm].fillna(0,inplace=True)\n",
    "    if cv.max()<=255:\n",
    "        df[nm] = df[nm].astype('uint8')\n",
    "        test[nm] = test[nm].astype('uint8')\n",
    "    else:\n",
    "        df[nm] = df[nm].astype('uint16')\n",
    "        test[nm] = test[nm].astype('uint16')        \n",
    "    return\n",
    "\n",
    "test_df['Label0'] = -1\n",
    "test_df['Label1'] = -1\n",
    "comb = pd.concat([train_df,test_df],axis=0)\n",
    "# comb=train_df\n",
    "# comb['nonce_median_bin']=pd.cut(comb['nonce'], bins=1000).apply(lambda x:(x.left+x.right)/2)\n",
    "for col in list(comb.columns):\n",
    "    if \"Label\" in col:# or \"FE\" in col:\n",
    "        print(col)\n",
    "        continue\n",
    "    \n",
    "    encode_FE(comb,col,test_df)\n",
    "train_df = comb[:len(train_df)]; del comb\n",
    "print('Added new magic features!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_df.apply(pd.to_numeric)\n",
    "import lightgbm as lgb\n",
    "df=train_df.copy()\n",
    "df=df.drop(labels=['Label1'], axis=1)\n",
    "Y=df['Label0']\n",
    "df=df.drop(labels=['Label0'], axis=1)\n",
    "# for i in df.columns:\n",
    "#     if \"FE\" in df:\n",
    "#         del df[i]\n",
    "try:\n",
    "    df=df.drop(labels=['Label1_FE'], axis=1)\n",
    "    df=df.drop(labels=['Label0_FE'], axis=1)\n",
    "    df=df.drop(labels=['output_FE'], axis=1)\n",
    "    df=df.drop(labels=['output_FE_FE'], axis=1)\n",
    "#     df=df.drop(labels=['Label0_FE_FE'], axis=1)\n",
    "#     df=df.drop(labels=['Label0_FE_FE'], axis=1)\n",
    "    df=df.drop(labels=['nonce_median_bin_FE'], axis=1)\n",
    "    df=df.drop(labels=['nonce_median_bin_FE_FE'], axis=1)\n",
    "    \n",
    "except:\n",
    "    pass\n",
    "#scaling the features using Standard Scaler\n",
    "# sc=StandardScaler()\n",
    "# sc=MinMaxScaler()\n",
    "# sc.fit(df)\n",
    "# X=pd.DataFrame(sc.fit_transform(df))\n",
    "X=df\n",
    "\n",
    "# X['Label_1']=y_label1\n",
    "\n",
    "\n",
    "#train_test_split\n",
    "# X_train=X[:738391]\n",
    "# y_train=Y[:738391]\n",
    "# X_test=X[738391:]\n",
    "# y_test=Y[738391:]\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.1,random_state=101)\n",
    "# X_train=pd.DataFrame(sc.fit_transform(X_train))\n",
    "# X_test=pd.DataFrame(sc.transform(X_test))\n",
    "# y_train=y_train.apply(lambda x:int(x))\n",
    "# y_test=y_test.apply(lambda x:int(x))\n",
    "# sm = SMOTE(random_state = 2)\n",
    "# X_train, y_train = sm.fit_sample(X_train, y_train.ravel())\n",
    "#converting the dataset into proper LGB format \n",
    "d_train=lgb.Dataset(X_train, label=y_train,free_raw_data=False)\n",
    "d_test=lgb.Dataset(X_test, label=y_test,free_raw_data=False)\n",
    "#Specifying the parameter\n",
    "# params={}\n",
    "# params['learning_rate']=0.07\n",
    "# params['boosting_type']='gbdt' #GradientBoostingDecisionTree\n",
    "# params['objective']='binary' #Binary target feature\n",
    "# params['metric']='binary_logloss' #metric for binary classification\n",
    "# params['max_depth']=100\n",
    "# params['feature_fraction']=0.8\n",
    "# params['num_iterations']=10000\n",
    "#train the model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ori_train=X_train.copy()\n",
    "# ori_test=X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=16)\n",
    "# X_train=pca.fit(X_train)\n",
    "# X_test=pca.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: learning_rates\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=0.4 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.375, colsample_bytree=0.5 will be ignored. Current value: feature_fraction=0.375\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] Unknown parameter: learning_rates\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=0.4 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.375, colsample_bytree=0.5 will be ignored. Current value: feature_fraction=0.375\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[LightGBM] [Info] Total Bins 12586\n",
      "[LightGBM] [Info] Number of data points in the train set: 738390, number of used features: 26\n",
      "[LightGBM] [Warning] Unknown parameter: learning_rates\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=0.4 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.375, colsample_bytree=0.5 will be ignored. Current value: feature_fraction=0.375\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[100]\ttraining's auc: 0.984979\ttraining's focal_loss: 0.126225\tvalid_1's auc: 0.98368\tvalid_1's focal_loss: 0.129106\n",
      "[200]\ttraining's auc: 0.987207\ttraining's focal_loss: 0.119265\tvalid_1's auc: 0.984988\tvalid_1's focal_loss: 0.117967\n",
      "[300]\ttraining's auc: 0.988634\ttraining's focal_loss: 0.105443\tvalid_1's auc: 0.985723\tvalid_1's focal_loss: 0.113939\n",
      "[400]\ttraining's auc: 0.989319\ttraining's focal_loss: 0.103263\tvalid_1's auc: 0.985927\tvalid_1's focal_loss: 0.113234\n",
      "[500]\ttraining's auc: 0.990426\ttraining's focal_loss: 0.105363\tvalid_1's auc: 0.986217\tvalid_1's focal_loss: 0.111389\n",
      "[600]\ttraining's auc: 0.991098\ttraining's focal_loss: 0.10225\tvalid_1's auc: 0.986317\tvalid_1's focal_loss: 0.110925\n",
      "[700]\ttraining's auc: 0.991766\ttraining's focal_loss: 0.0974276\tvalid_1's auc: 0.986458\tvalid_1's focal_loss: 0.110073\n",
      "[800]\ttraining's auc: 0.992556\ttraining's focal_loss: 0.089536\tvalid_1's auc: 0.986565\tvalid_1's focal_loss: 0.109295\n",
      "[900]\ttraining's auc: 0.993206\ttraining's focal_loss: 0.0907588\tvalid_1's auc: 0.986624\tvalid_1's focal_loss: 0.10895\n",
      "[1000]\ttraining's auc: 0.993906\ttraining's focal_loss: 0.0836032\tvalid_1's auc: 0.986711\tvalid_1's focal_loss: 0.10841\n",
      "[1100]\ttraining's auc: 0.994502\ttraining's focal_loss: 0.0811409\tvalid_1's auc: 0.986755\tvalid_1's focal_loss: 0.108205\n",
      "[1200]\ttraining's auc: 0.995115\ttraining's focal_loss: 0.0810846\tvalid_1's auc: 0.986786\tvalid_1's focal_loss: 0.107937\n",
      "[1300]\ttraining's auc: 0.995633\ttraining's focal_loss: 0.0784701\tvalid_1's auc: 0.986808\tvalid_1's focal_loss: 0.107855\n",
      "[1400]\ttraining's auc: 0.996001\ttraining's focal_loss: 0.0762875\tvalid_1's auc: 0.986873\tvalid_1's focal_loss: 0.10754\n",
      "[1500]\ttraining's auc: 0.99638\ttraining's focal_loss: 0.0718548\tvalid_1's auc: 0.986867\tvalid_1's focal_loss: 0.107596\n",
      "[1600]\ttraining's auc: 0.996806\ttraining's focal_loss: 0.069245\tvalid_1's auc: 0.986921\tvalid_1's focal_loss: 0.107318\n",
      "[1700]\ttraining's auc: 0.997113\ttraining's focal_loss: 0.068973\tvalid_1's auc: 0.986926\tvalid_1's focal_loss: 0.107262\n",
      "[1800]\ttraining's auc: 0.997223\ttraining's focal_loss: 0.0690306\tvalid_1's auc: 0.98696\tvalid_1's focal_loss: 0.107146\n",
      "[1900]\ttraining's auc: 0.997456\ttraining's focal_loss: 0.0665827\tvalid_1's auc: 0.986981\tvalid_1's focal_loss: 0.10701\n"
     ]
    }
   ],
   "source": [
    "from scipy import special\n",
    "\n",
    "def logloss_objective(preds, train_data):\n",
    "    y = train_data.get_label()\n",
    "    p = special.expit(preds)\n",
    "    grad = p - y\n",
    "    hess = p * (1 - p)\n",
    "    return grad, hess\n",
    "\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "from scipy import special\n",
    "\n",
    "class FocalLoss:\n",
    "\n",
    "    def __init__(self, gamma, alpha=None):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def at(self, y):\n",
    "        if self.alpha is None:\n",
    "            return np.ones_like(y)\n",
    "        return np.where(y, self.alpha, 1 - self.alpha)\n",
    "\n",
    "    def pt(self, y, p):\n",
    "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
    "        return np.where(y, p, 1 - p)\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        at = self.at(y_true)\n",
    "        pt = self.pt(y_true, y_pred)\n",
    "        return -at * (1 - pt) ** self.gamma * np.log(pt)\n",
    "\n",
    "    def grad(self, y_true, y_pred):\n",
    "        y = 2 * y_true - 1  # {0, 1} -> {-1, 1}\n",
    "        at = self.at(y_true)\n",
    "        pt = self.pt(y_true, y_pred)\n",
    "        g = self.gamma\n",
    "        return at * y * (1 - pt) ** g * (g * pt * np.log(pt) + pt - 1)\n",
    "\n",
    "    def hess(self, y_true, y_pred):\n",
    "        y = 2 * y_true - 1  # {0, 1} -> {-1, 1}\n",
    "        at = self.at(y_true)\n",
    "        pt = self.pt(y_true, y_pred)\n",
    "        g = self.gamma\n",
    "\n",
    "        u = at * y * (1 - pt) ** g\n",
    "        du = -at * y * g * (1 - pt) ** (g - 1)\n",
    "        v = g * pt * np.log(pt) + pt - 1\n",
    "        dv = g * np.log(pt) + g + 1\n",
    "\n",
    "        return (du * v + u * dv) * y * (pt * (1 - pt))\n",
    "\n",
    "    def init_score(self, y_true):\n",
    "        res = optimize.minimize_scalar(\n",
    "            lambda p: self(y_true, p).sum(),\n",
    "            bounds=(0, 1),\n",
    "            method='bounded'\n",
    "        )\n",
    "        p = res.x\n",
    "        log_odds = np.log(p / (1 - p))\n",
    "        return log_odds\n",
    "\n",
    "    def lgb_obj(self, preds, train_data):\n",
    "        y = train_data.get_label()\n",
    "        p = special.expit(preds)\n",
    "        return self.grad(y, p), self.hess(y, p)\n",
    "\n",
    "    def lgb_eval(self, preds, train_data):\n",
    "        y = train_data.get_label()\n",
    "        p = special.expit(preds)\n",
    "        is_higher_better = False\n",
    "        return 'focal_loss', self(y, p).mean(), is_higher_better\n",
    "\n",
    "import lightgbm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import optimize\n",
    "from scipy import special\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "\n",
    "fl = FocalLoss(alpha=None, gamma=0)\n",
    "\n",
    "y_train=y_train.astype(np.int8)\n",
    "y_test=y_test.astype(np.int8)\n",
    "\n",
    "Y=Y.astype(np.int8)\n",
    "\n",
    "fit = lightgbm.Dataset(\n",
    "    X_train, y_train,\n",
    "    init_score=np.full_like(y_train, fl.init_score(y_train), dtype=float),free_raw_data=False\n",
    ")\n",
    "\n",
    "# X_test_new=X_test.copy()\n",
    "# y_test=X_test_new['Label0']\n",
    "# del X_test_new['Label1']\n",
    "# del X_test_new['Label0']\n",
    "\n",
    "# y_test=y_test.apply(lambda x:int(x))\n",
    "\n",
    "val = lightgbm.Dataset(\n",
    "    X_test, y_test,\n",
    "    init_score=np.full_like(y_test, fl.init_score(y_test), dtype=float),\n",
    "    reference=fit,free_raw_data=False\n",
    ")\n",
    "\n",
    "len(X_train),len(X_test),len(train_df)\n",
    "\n",
    "X_train.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "param = {'objective': 'binary',\n",
    "         'boosting': 'dart',#'dart',  #use dart mode for model 2\n",
    "         'metric': 'auc',\n",
    "         'boosting_type':'gbdt',\n",
    "         'learning_rates': 0.005, \n",
    "         'num_iterations': 1900,\n",
    "         'num_leaves':256, #.2763756462785785\n",
    "         'max_depth': -1,\n",
    "         'min_data_in_leaf': 10,\n",
    "         'bagging_fraction': 1.0,\n",
    "         'bagging_freq': 1,\n",
    "         'bagging_seed': 3,\n",
    "         'feature_fraction': 0.375,#0.375->0.986511 #0.1->0.986489\n",
    "         'feature_fraction_seed': 2,\n",
    "         'early_stopping_round': 200,\n",
    "         'max_bin': 1000,\n",
    "         'lambda': 5.8849054495567423, \n",
    "        'alpha': 0.001054193185317787, \n",
    "        'colsample_bytree': 0.5, \n",
    "        'subsample': 0.4, \n",
    "#         'max_depth': 500, \n",
    "        'random_state': 101,\n",
    "        'min_child_weight': 5,\n",
    "         'n_estimators':1000,\n",
    "         'force_col_wise':'true',\n",
    "#          'first_metric_only':True,\n",
    "         'n_jobs':8,\n",
    "         'num_boost_round':1900\n",
    "         }\n",
    "# param = {\n",
    "#     'learning_rate': 0.04,\n",
    "#     'num_leaves': 3,\n",
    "#     'metric':['auc'],\n",
    "#     'boost_from_average':'false',\n",
    "#     'feature_fraction': 1.0,\n",
    "#     'max_depth': -1,\n",
    "#     'objective': 'binary',\n",
    "#     'verbosity': -10,\n",
    "#     'n_jobs':32}\n",
    "\n",
    "\n",
    "\n",
    "# clf1=lgb.train(param,verbose_eval=100, train_set=d_train, valid_sets=[d_train,d_test],num_boost_round=1700) #train the model on 100 epocs\n",
    "# clf=lgb.train(param,verbose_eval=100, train_set=fit, valid_sets=(fit, val),num_boost_round=1900,fobj=fl.lgb_obj,feval=fl.lgb_eval)\n",
    "clf_e=lgb.train(param,verbose_eval=100, train_set=fit, valid_sets=(fit, val),num_boost_round=1900,fobj=fl.lgb_obj,feval=fl.lgb_eval)\n",
    "\n",
    "#prediction on the test set\n",
    "# y_pred=clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = special.expit(fl.init_score(y_train) + clf_e.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         ... 0.96997948 0.9700088  1.        ] [0.00000000e+00 7.23379630e-05 1.06336806e-01 ... 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00] [1.99994077e+00 9.99940774e-01 9.84352430e-01 ... 6.46431200e-06\n",
      " 6.46140743e-06 7.64947176e-07]\n",
      "0.9869233608230316\n"
     ]
    }
   ],
   "source": [
    "# y_pred = special.expit(fl.init_score(y_test.astype(np.int8)) + clf_exp_new4.predict(X_test))\n",
    "from sklearn import metrics\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, abs(y_pred), pos_label=1)\n",
    "print(fpr,tpr,thresholds)\n",
    "print(metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del test_df['Label0']\n",
    "    del test_df['Label1']\n",
    "except:\n",
    "    pass\n",
    "binaryPredictions=special.expit(fl.init_score(y_train) + clf_e.predict(test_df.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: learning_rates\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=0.4 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=0.5 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Warning] Unknown parameter: learning_rates\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=0.4 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=0.5 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Info] Total Bins 12586\n",
      "[LightGBM] [Info] Number of data points in the train set: 738390, number of used features: 26\n",
      "[LightGBM] [Warning] Unknown parameter: learning_rates\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=0.4 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=0.5 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "[LightGBM] [Info] Start training from score 0.698364\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's rmse: 0.0262036\tvalid_1's rmse: 0.0427796\n",
      "[200]\ttraining's rmse: 0.0220838\tvalid_1's rmse: 0.0427006\n",
      "[300]\ttraining's rmse: 0.0193445\tvalid_1's rmse: 0.0426197\n",
      "[400]\ttraining's rmse: 0.0171452\tvalid_1's rmse: 0.0426459\n",
      "Early stopping, best iteration is:\n",
      "[271]\ttraining's rmse: 0.0202813\tvalid_1's rmse: 0.042587\n",
      "Evaluated only: rmse\n",
      "RMSLE: 0.04258701229951673\n",
      "MAE: 0.01762585931593206\n"
     ]
    }
   ],
   "source": [
    "ori=train_df.copy()\n",
    "train_df=ori.copy()\n",
    "eps=1\n",
    "train_df['Label1']=abs(train_df['Label1'])\n",
    "y_ori=train_df['Label1'].copy()\n",
    "train_df['Label1'] = np.log1p(train_df['Label1']+eps)\n",
    "\n",
    "X = train_df.drop(labels=['Label1'], axis=1)\n",
    "X = X.drop(labels=['Label0'], axis=1)\n",
    "y = train_df['Label1'].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=0.10, random_state=101)\n",
    "from sklearn.model_selection import train_test_split\n",
    "_, _, _, y_cv_ori = train_test_split(X, y_ori, test_size=0.10, random_state=101)\n",
    "\n",
    "X_train.shape, y_train.shape, X_cv.shape, y_cv.shape\n",
    "\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from math import sqrt\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_cv, label=y_cv)\n",
    "\n",
    "\n",
    "# param = {'objective': 'regression',\n",
    "#          'boosting': 'gbdt',  #use dart mode for model 2\n",
    "#          'metric': 'l2_root',\n",
    "#          'boosting_type':'gbdt',\n",
    "#          'learning_rates': 0.001, \n",
    "#          'num_iterations': 10000,\n",
    "#          'num_leaves':512, #.2763756462785785\n",
    "#          #'max_depth': -1,\n",
    "#          'min_data_in_leaf': 2,\n",
    "#          'bagging_fraction': 0.90,\n",
    "#          'bagging_freq': 1,\n",
    "#          'bagging_seed': 3,\n",
    "#          'feature_fraction': 0.90,\n",
    "#          'feature_fraction_seed': 2,\n",
    "#          'early_stopping_round': 200,\n",
    "#          'max_bin': 1000,\n",
    "#          'lambda': 2.8849054495567423, \n",
    "#         'alpha': 0.001054193185317787, \n",
    "#         'colsample_bytree': 0.5, \n",
    "#         'subsample': 0.4, \n",
    "#         'max_depth': 1000, \n",
    "#         'random_state': 101,\n",
    "#         'min_child_weight': 5,\n",
    "#          'n_estimators':100,\n",
    "#          'force_col_wise':'true'\n",
    "#          }\n",
    "param = {'objective': 'regression',\n",
    "         'boosting': 'dart',  #use dart mode for model 2\n",
    "         'metric': 'l2_root',\n",
    "         'boosting_type':'gbdt',\n",
    "         'learning_rates': 0.005, \n",
    "         'num_iterations': 2200,\n",
    "         'num_leaves':256, #.2763756462785785\n",
    "         'max_depth': -1,\n",
    "         'min_data_in_leaf': 10,\n",
    "         'bagging_fraction': 1.0,\n",
    "         'bagging_freq': 1,\n",
    "         'bagging_seed': 3,\n",
    "         'feature_fraction': 1.0,\n",
    "         'feature_fraction_seed': 2,\n",
    "         'early_stopping_round': 200,\n",
    "         'max_bin': 1000,\n",
    "         'lambda': 2.8849054495567423, \n",
    "        'alpha': 0.001054193185317787, \n",
    "        'colsample_bytree': 0.5, \n",
    "        'subsample': 0.4, \n",
    "#         'max_depth': 500, \n",
    "        'random_state': 101,\n",
    "        'min_child_weight': 5,\n",
    "         'n_estimators':1000,\n",
    "         'force_col_wise':'true',\n",
    "         'first_metric_only':True,\n",
    "         'n_jobs':31,\n",
    "         'num_boost_round':2200\n",
    "         }\n",
    "\n",
    "\n",
    "\n",
    "# lgbm = lgb.train(params=param, verbose_eval=100, train_set=train_data, valid_sets=[train_data,test_data])\n",
    "\n",
    "# y_pred_lgbm = lgbm.predict(X_cv)\n",
    "# print('RMSLE:', sqrt(mean_squared_log_error(np.expm1(y_cv), np.expm1(y_pred_lgbm))))\n",
    "# print('MAE:', MAE(np.expm1(y_cv), np.expm1(y_pred_lgbm)))\n",
    "param['boosting']='gbdt'\n",
    "lgbm1 = lgb.train(params=param, verbose_eval=100, train_set=train_data, valid_sets=[train_data,test_data])\n",
    "y_pred_lgbm1 = lgbm1.predict(X_cv)\n",
    "print('RMSLE:', sqrt(mean_squared_log_error(np.expm1(y_cv), np.expm1(y_pred_lgbm1))))\n",
    "print('MAE:', MAE(np.expm1(y_cv), np.expm1(y_pred_lgbm1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = lgbm1.predict(test_df.values)\n",
    "regressionPredictions=abs(np.expm1(y_pred_test)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVING SUBMISSION FILE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = csv.writer(open('submission.csv', 'w', encoding='UTF8'))\n",
    "for x, y in zip(binaryPredictions, regressionPredictions):\n",
    "    submission.writerow([x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
